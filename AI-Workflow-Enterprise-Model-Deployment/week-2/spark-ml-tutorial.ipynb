{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Spark Machine Learning\n",
    "\n",
    "Keep the main [Spark ML documentation](https://spark.apache.org/docs/latest/ml-pipeline.html) as you go through this tutorial.  MLlib is Sparkâ€™s machine learning (ML) library. **Spark ML** is not an official name, but we will use it to refer to the MLlib DataFrame-based API that embraces ML pipelines. Before we get into Spark ML by demonstrating a couple of examples we will first review Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and **puts a schema on them**.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as **RDDs with schema**.\n",
    "\n",
    "What are **schemas**?\n",
    "- Schemas are metadata about your data.\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using **column names** instead of column positions\n",
    "- Schemas enable **queries** using SQL and DataFrame syntax\n",
    "- Schemas make your data more **structured**.\n",
    "\n",
    "See the [Spark SQL documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html) as a main point of reference for Spark SQL, DataFrames and Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module.  \n",
    "\n",
    "<center>\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th>Types</th>\n",
    "      <th>Python Equivalent</th>\n",
    "    </tr>\n",
    "  <tr>\n",
    "      <td>StringType</td>\n",
    "      <td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td>IntegerType</td>\n",
    "      <td>integer</td>\n",
    "   <tr>\n",
    "      <td>FloatType</td>\n",
    "      <td>float</td> \n",
    "  <tr>\n",
    "      <td>ArrayType</td>\n",
    "      <td>array or list</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td>MapType</td>\n",
    "      <td>dict</td>\n",
    "   </tr>       \n",
    "</table>\n",
    "</center>\n",
    "\n",
    "First we initialize the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"spark-ml-examples\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `local[4]` will create a `local` cluster made of the driver using all 4 cores.  Lets start with a very small file to to demonstrate the different ways to create Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(111, '10/13/2019', 4, 'US', 1),\n",
       " (122, '10/15/2019', 5, 'SG', 1),\n",
       " (102, '10/16/2019', 11, 'US', 1),\n",
       " (144, '10/25/2019', 14, 'US', 2),\n",
       " (121, '10/26/2019', 7, 'SG', 1),\n",
       " (155, '10/27/2019', 9, 'US', 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(\"..\",\"data\")\n",
    "def casting_function(args):\n",
    "    user_id, date, num_streams, country, invoice_item = args\n",
    "    return((int(user_id), date, int(num_streams), country, int(invoice_item)))\n",
    "\n",
    "rdd_aavail = sc.textFile(os.path.join(data_dir,'example-data.csv'))\\\n",
    "                         .map(lambda rowstr : rowstr.split(\",\"))\\\n",
    "                         .filter(lambda row: not row[0].startswith('#'))\\\n",
    "                         .map(casting_function)\n",
    "\n",
    "rdd_aavail.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can create a Spark DataFrame using a schema that you have defined or it can be inferred.  To create your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-------+-------------+\n",
      "|user_id|      date|num_streams|country|invoice_items|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "|    111|10/13/2019|          4|     US|            1|\n",
      "|    122|10/15/2019|          5|     SG|            1|\n",
      "|    102|10/16/2019|         11|     US|            1|\n",
      "|    144|10/25/2019|         14|     US|            2|\n",
      "|    121|10/26/2019|          7|     SG|            1|\n",
      "|    155|10/27/2019|          9|     US|            3|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_items: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('num_streams',IntegerType(),True),\n",
    "    StructField('country',StringType(),True),\n",
    "    StructField('invoice_items',IntegerType(),True) ])\n",
    "    \n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_aavail,schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also read the data directly from a file and **infer** the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv(os.path.join(data_dir,'example-data.csv'),\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn the DataFrame into a Panda DataFrame, but be careful since this 'action' will put all the data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user</th>\n",
       "      <th>date</th>\n",
       "      <th>num_streams</th>\n",
       "      <th>country</th>\n",
       "      <th>invoice_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>10/13/2019</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>10/15/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>10/16/2019</td>\n",
       "      <td>11</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>144</td>\n",
       "      <td>10/25/2019</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>7</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>10/27/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #user        date  num_streams country  invoice_item\n",
       "0    111  10/13/2019            4      US             1\n",
       "1    122  10/15/2019            5      SG             1\n",
       "2    102  10/16/2019           11      US             1\n",
       "3    144  10/25/2019           14      US             2\n",
       "4    121  10/26/2019            7      SG             1\n",
       "5    155  10/27/2019            9      US             3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common operations that you might perform on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|summary|             #user|      date|      num_streams|country|      invoice_item|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|  count|                 6|         6|                6|      6|                 6|\n",
      "|   mean|125.83333333333333|      null|8.333333333333334|   null|               1.5|\n",
      "| stddev|20.034137532388726|      null|3.777124126457412|   null|0.8366600265340756|\n",
      "|    min|               102|10/13/2019|                4|     SG|                 1|\n",
      "|    max|               155|10/27/2019|               14|     US|                 3|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+-----------------+\n",
      "|summary|      num_streams|\n",
      "+-------+-----------------+\n",
      "|  count|                6|\n",
      "|   mean|8.333333333333334|\n",
      "| stddev|3.777124126457412|\n",
      "|    min|                4|\n",
      "|    max|               14|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df.describe(\"num_streams\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "\n",
    "Lets read in in the AAVAIL dataset that we have been working with to demonstrate the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|summary|      customer_id|     is_subscriber|      country|               age| customer_name| subscriber_type|      num_streams|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|  count|             1000|              1000|         1000|              1000|          1000|            1000|             1000|\n",
      "|   mean|            500.5|             0.711|         null|            25.325|          null|            null|           17.695|\n",
      "| stddev|288.8194360957494|0.4535247343692345|         null|12.184655959067568|          null|            null|4.798020007877829|\n",
      "|    min|                1|                 0|    singapore|               -50|Aaliyah Duarte|    aavail_basic|                1|\n",
      "|    max|             1000|                 1|united_states|                50|   Zoie Cortes|aavail_unlimited|               29|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aavail = spark.read.csv(os.path.join(data_dir,'aavail-target.csv'),\n",
    "                           header=True,       \n",
    "                           quote='\"',         \n",
    "                           sep=\",\",          \n",
    "                           inferSchema=True)\n",
    "df_aavail.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Remove one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|summary|     is_subscriber|      country|               age| subscriber_type|      num_streams|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|  count|              1000|         1000|              1000|            1000|             1000|\n",
      "|   mean|             0.711|         null|            25.325|            null|           17.695|\n",
      "| stddev|0.4535247343692345|         null|12.184655959067568|            null|4.798020007877829|\n",
      "|    min|                 0|    singapore|               -50|    aavail_basic|                1|\n",
      "|    max|                 1|united_states|                50|aavail_unlimited|               29|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "\n",
      "+----------------+-----+\n",
      "| subscriber_type|count|\n",
      "+----------------+-----+\n",
      "|  aavail_premium|  331|\n",
      "|aavail_unlimited|  302|\n",
      "|    aavail_basic|  367|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['customer_id','customer_name']\n",
    "df_aavail = df_aavail.drop(*columns_to_drop)\n",
    "df_aavail.describe().show()\n",
    "df_aavail.groupBy(\"subscriber_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on a feature matrix\n",
    "\n",
    "The following example demonstrates how to deal with categorical features and scale continuous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## scale the continuous features\n",
    "va = VectorAssembler(inputCols=[\"age\", \"num_streams\"], outputCol=\"cont_features\")\n",
    "ss = standardScaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaled\")\n",
    "\n",
    "## categorical variable transformation\n",
    "cat_cols = [\"country\",\"subscriber_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_oh\") for column in cat_cols]\n",
    "\n",
    "## assemple the features for input into the ML model\n",
    "assembler = VectorAssembler(inputCols=[\"cont_scaled\", \"country_oh\",\"subscriber_type_oh\"], outputCol=\"features\")\n",
    "\n",
    "## setup a model\n",
    "gbt = GBTClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\")\n",
    "paramMap = {gbt.maxIter: 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the pipeline and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[1.72347910934425...|            1|[-0.7104551723677...|[0.19451890992497...|       1.0|\n",
      "|(5,[0,1],[2.46211...|            0|[0.77502044150328...|[0.82491963653378...|       0.0|\n",
      "|[1.72347910934425...|            0|[-0.7104551723677...|[0.19451890992497...|       1.0|\n",
      "|[1.64140867556596...|            1|[-0.9291971347680...|[0.13489032271470...|       1.0|\n",
      "|[1.72347910934425...|            1|[0.00233192525665...|[0.50116596051488...|       0.0|\n",
      "|[1.72347910934425...|            1|[-0.7703335442768...|[0.17643832065912...|       1.0|\n",
      "|[3.93938082135830...|            0|[0.74986801956411...|[0.81753510406590...|       0.0|\n",
      "|[3.85731038758001...|            1|[-0.9215852644165...|[0.13667675103731...|       1.0|\n",
      "|[1.72347910934425...|            0|[-0.4368188087314...|[0.29449795362552...|       1.0|\n",
      "|[2.13383127823575...|            0|[-0.8756884670117...|[0.14787361056083...|       1.0|\n",
      "|[1.14898607289617...|            1|[-1.2076283318498...|[0.08201667364617...|       1.0|\n",
      "|(5,[0,1],[3.20074...|            0|[1.36797713189510...|[0.93911518070094...|       0.0|\n",
      "|[1.64140867556596...|            1|[-0.8905262171262...|[0.14417322842494...|       1.0|\n",
      "|[1.55933824178766...|            1|[-1.0286924396352...|[0.11330830445383...|       1.0|\n",
      "|[1.80554954312255...|            0|[-0.6576928685452...|[0.21158701096329...|       1.0|\n",
      "|[1.80554954312255...|            1|[-1.4203671689725...|[0.05516225204124...|       1.0|\n",
      "|[2.21590171201404...|            1|[-0.6766933461545...|[0.20531724057277...|       1.0|\n",
      "|[3.61109908624511...|            0|[0.46428590396626...|[0.71678544647694...|       0.0|\n",
      "|[2.29797214579234...|            1|[-1.0136431193885...|[0.11636767755533...|       1.0|\n",
      "|[1.64140867556596...|            1|[-1.4405485892111...|[0.05309594675816...|       1.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## run the whole pipeline\n",
    "pipe = Pipeline(stages=indexers+encoders+[va,ss,assembler,gbt])\n",
    "_result = pipe.fit(df_aavail,paramMap).transform(df_aavail)\n",
    "result = _result.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\",\"prediction\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now the same procedure with a train-test split, cross-validations and grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train, test = df_aavail.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "    .addGrid(gbt.stepSize, [0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "pipe = Pipeline(stages=indexers+encoders+[va,ss,assembler])\n",
    "pipeline_model = pipe.fit(train)\n",
    "prepped_train = pipeline_model.transform(train)\n",
    "prepped_test = pipeline_model.transform(test)\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"is_subscriber\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(prepped_train)\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-4.0596848816199...|            0|[0.41267325985154...|[0.69537007776985...|       0.0|\n",
      "|[-3.9733086075428...|            0|[0.37306874626530...|[0.67833649631054...|       0.0|\n",
      "|(5,[0,1],[-3.9733...|            0|[1.43301173324304...|[0.94614107034632...|       0.0|\n",
      "|(5,[0,1],[1.46839...|            0|[0.34762259838570...|[0.66713272935263...|       0.0|\n",
      "|(5,[0,1],[1.46839...|            0|[0.87976021534760...|[0.85314958735639...|       0.0|\n",
      "|(5,[0,1],[1.64114...|            0|[0.28398868883873...|[0.63829633473669...|       0.0|\n",
      "|(5,[0,1],[1.72752...|            0|[0.56511254555211...|[0.75588043646855...|       0.0|\n",
      "|(5,[0,1],[1.72752...|            0|[0.41992091829299...|[0.69843190391389...|       0.0|\n",
      "|[1.81390175561740...|            0|[0.11219152959031...|[0.55586158576770...|       0.0|\n",
      "|[1.81390175561740...|            0|[0.75709265372054...|[0.81968064110814...|       0.0|\n",
      "|(5,[0,1],[1.81390...|            0|[0.64691270787757...|[0.78479397664775...|       0.0|\n",
      "|(5,[0,1],[1.90027...|            0|[0.21700232410837...|[0.60682953489612...|       0.0|\n",
      "|(5,[0,1],[1.90027...|            0|[0.41764494919560...|[0.69747228781921...|       0.0|\n",
      "|(5,[0,1],[1.90027...|            0|[0.91853686119791...|[0.86260225401788...|       0.0|\n",
      "|(5,[0,1],[1.98665...|            0|[0.99725212832784...|[0.88021885154417...|       0.0|\n",
      "|[2.24578312600249...|            0|[0.14917445536985...|[0.57403884479451...|       0.0|\n",
      "|[2.50491194823355...|            0|[0.68289278967225...|[0.79669839708297...|       0.0|\n",
      "|(5,[0,1],[2.59128...|            0|[0.72928657283184...|[0.81131434412240...|       0.0|\n",
      "|[2.76404077046461...|            0|[-0.0731655402872...|[0.46348236862940...|       1.0|\n",
      "|[2.85041704454163...|            0|[1.32950710259324...|[0.93456440731875...|       0.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(prepped_test)\n",
    "result = prediction.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\",\"prediction\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
